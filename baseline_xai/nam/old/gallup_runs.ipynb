{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "df955ce39d0f31d56d4bb2fe0a613e5326ba60723fd33d8303a3aede8f65715c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nam.config import defaults\n",
    "from nam.types import Config\n",
    "from nam.utils.args import parse_args\n",
    "from nam.data import NAMDataset\n",
    "from nam.models import DNN, FeatureNN, NAM, get_num_units\n",
    "from nam.engine import Engine\n",
    "from nam.utils import graphing\n",
    "\n",
    "from main import get_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WP16                    0\nwgt                     0\ncountry                 0\nincome_2                0\nWP1219                  1\nWP1220                185\nyear                    0\nweo_gdpc_con_ppp    24122\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "\n",
    "features_columns = [\"income_2\", \"WP1219\", \"WP1220\", \"weo_gdpc_con_ppp\"]\n",
    "targets_column = [\"WP16\"]\n",
    "weights_column = [\"wgt\"]\n",
    "\n",
    "data = pd.read_csv('data/GALLUP.csv')\n",
    "missing = data.isnull().sum()\n",
    "print(missing)\n",
    "data = data.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data[features_columns])\n",
    "y = np.array(data[targets_column])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2137)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype('float32'))\n",
    "X_test = torch.from_numpy(X_test.astype('float32'))\n",
    "y_train = torch.from_numpy(y_train.reshape(-1, 1).astype('float32'))\n",
    "y_test = torch.from_numpy(y_test.reshape(-1, 1).astype('float32'))\n",
    "\n",
    "dataset_train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "batch_size = 128\n",
    "dataset_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (layer1): Linear(in_features=4, out_features=32, bias=True)\n",
       "  (layer2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (layer3): Linear(in_features=16, out_features=12, bias=True)\n",
       "  (layer4): Linear(in_features=12, out_features=1, bias=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (dropout3): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(4, 32)\n",
    "        self.layer2 = nn.Linear(32, 16)\n",
    "        self.layer3 = nn.Linear(16, 12)\n",
    "        self.layer4 = nn.Linear(12, 1)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.dropout3 = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "nn_model = NeuralNetwork()\n",
    "nn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_obj = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(nn_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(8.6204, grad_fn=<MseLossBackward>)\n",
      "tensor(8.3854, grad_fn=<MseLossBackward>)\n",
      "tensor(9.6203, grad_fn=<MseLossBackward>)\n",
      "tensor(10.5477, grad_fn=<MseLossBackward>)\n",
      "tensor(11.7218, grad_fn=<MseLossBackward>)\n",
      "tensor(6.1141, grad_fn=<MseLossBackward>)\n",
      "tensor(6.0120, grad_fn=<MseLossBackward>)\n",
      "tensor(7.7470, grad_fn=<MseLossBackward>)\n",
      "tensor(7.3754, grad_fn=<MseLossBackward>)\n",
      "tensor(6.4906, grad_fn=<MseLossBackward>)\n",
      "tensor(4.9904, grad_fn=<MseLossBackward>)\n",
      "tensor(6.1464, grad_fn=<MseLossBackward>)\n",
      "tensor(4.9804, grad_fn=<MseLossBackward>)\n",
      "tensor(5.0341, grad_fn=<MseLossBackward>)\n",
      "tensor(5.7309, grad_fn=<MseLossBackward>)\n",
      "tensor(4.4846, grad_fn=<MseLossBackward>)\n",
      "tensor(4.7525, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4566, grad_fn=<MseLossBackward>)\n",
      "tensor(5.6220, grad_fn=<MseLossBackward>)\n",
      "tensor(4.7918, grad_fn=<MseLossBackward>)\n",
      "tensor(4.4297, grad_fn=<MseLossBackward>)\n",
      "tensor(5.6841, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4914, grad_fn=<MseLossBackward>)\n",
      "tensor(4.8070, grad_fn=<MseLossBackward>)\n",
      "tensor(6.0818, grad_fn=<MseLossBackward>)\n",
      "tensor(6.4288, grad_fn=<MseLossBackward>)\n",
      "tensor(4.6495, grad_fn=<MseLossBackward>)\n",
      "tensor(5.7356, grad_fn=<MseLossBackward>)\n",
      "tensor(6.3984, grad_fn=<MseLossBackward>)\n",
      "tensor(5.2211, grad_fn=<MseLossBackward>)\n",
      "tensor(4.8260, grad_fn=<MseLossBackward>)\n",
      "tensor(5.0056, grad_fn=<MseLossBackward>)\n",
      "tensor(5.0374, grad_fn=<MseLossBackward>)\n",
      "tensor(5.2714, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3414, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4465, grad_fn=<MseLossBackward>)\n",
      "tensor(3.5250, grad_fn=<MseLossBackward>)\n",
      "tensor(6.1561, grad_fn=<MseLossBackward>)\n",
      "tensor(5.1493, grad_fn=<MseLossBackward>)\n",
      "tensor(5.1227, grad_fn=<MseLossBackward>)\n",
      "tensor(5.8754, grad_fn=<MseLossBackward>)\n",
      "tensor(4.8183, grad_fn=<MseLossBackward>)\n",
      "tensor(5.6462, grad_fn=<MseLossBackward>)\n",
      "tensor(6.4684, grad_fn=<MseLossBackward>)\n",
      "tensor(4.8273, grad_fn=<MseLossBackward>)\n",
      "tensor(6.3369, grad_fn=<MseLossBackward>)\n",
      "tensor(6.1850, grad_fn=<MseLossBackward>)\n",
      "tensor(5.5400, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4522, grad_fn=<MseLossBackward>)\n",
      "tensor(5.2493, grad_fn=<MseLossBackward>)\n",
      "tensor(4.5142, grad_fn=<MseLossBackward>)\n",
      "tensor(5.7322, grad_fn=<MseLossBackward>)\n",
      "tensor(5.9124, grad_fn=<MseLossBackward>)\n",
      "tensor(4.8849, grad_fn=<MseLossBackward>)\n",
      "tensor(4.6544, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3038, grad_fn=<MseLossBackward>)\n",
      "tensor(4.7037, grad_fn=<MseLossBackward>)\n",
      "tensor(5.1002, grad_fn=<MseLossBackward>)\n",
      "tensor(4.9673, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3967, grad_fn=<MseLossBackward>)\n",
      "tensor(5.6242, grad_fn=<MseLossBackward>)\n",
      "tensor(5.6861, grad_fn=<MseLossBackward>)\n",
      "tensor(5.1507, grad_fn=<MseLossBackward>)\n",
      "tensor(5.1731, grad_fn=<MseLossBackward>)\n",
      "tensor(6.1270, grad_fn=<MseLossBackward>)\n",
      "tensor(6.5184, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3462, grad_fn=<MseLossBackward>)\n",
      "tensor(4.4150, grad_fn=<MseLossBackward>)\n",
      "tensor(5.1524, grad_fn=<MseLossBackward>)\n",
      "tensor(7.1797, grad_fn=<MseLossBackward>)\n",
      "tensor(6.1815, grad_fn=<MseLossBackward>)\n",
      "tensor(6.1971, grad_fn=<MseLossBackward>)\n",
      "tensor(5.2605, grad_fn=<MseLossBackward>)\n",
      "tensor(4.4873, grad_fn=<MseLossBackward>)\n",
      "tensor(5.0525, grad_fn=<MseLossBackward>)\n",
      "tensor(5.8311, grad_fn=<MseLossBackward>)\n",
      "tensor(4.7179, grad_fn=<MseLossBackward>)\n",
      "tensor(4.8909, grad_fn=<MseLossBackward>)\n",
      "tensor(4.2532, grad_fn=<MseLossBackward>)\n",
      "tensor(4.9810, grad_fn=<MseLossBackward>)\n",
      "tensor(6.0468, grad_fn=<MseLossBackward>)\n",
      "tensor(4.6101, grad_fn=<MseLossBackward>)\n",
      "tensor(5.8357, grad_fn=<MseLossBackward>)\n",
      "tensor(5.2445, grad_fn=<MseLossBackward>)\n",
      "tensor(5.7831, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4134, grad_fn=<MseLossBackward>)\n",
      "tensor(5.9033, grad_fn=<MseLossBackward>)\n",
      "tensor(6.0102, grad_fn=<MseLossBackward>)\n",
      "tensor(4.8047, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4272, grad_fn=<MseLossBackward>)\n",
      "tensor(5.7987, grad_fn=<MseLossBackward>)\n",
      "tensor(5.7250, grad_fn=<MseLossBackward>)\n",
      "tensor(5.6113, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3742, grad_fn=<MseLossBackward>)\n",
      "tensor(4.2163, grad_fn=<MseLossBackward>)\n",
      "tensor(4.7886, grad_fn=<MseLossBackward>)\n",
      "tensor(4.6865, grad_fn=<MseLossBackward>)\n",
      "tensor(4.7595, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4756, grad_fn=<MseLossBackward>)\n",
      "tensor(5.6165, grad_fn=<MseLossBackward>)\n",
      "tensor(5.3762, grad_fn=<MseLossBackward>)\n",
      "tensor(4.5963, grad_fn=<MseLossBackward>)\n",
      "tensor(4.4199, grad_fn=<MseLossBackward>)\n",
      "tensor(7.4045, grad_fn=<MseLossBackward>)\n",
      "tensor(5.1534, grad_fn=<MseLossBackward>)\n",
      "tensor(5.9562, grad_fn=<MseLossBackward>)\n",
      "tensor(5.5805, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-56e5bb6c50d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    i = 0\n",
    "    for X, y in dataset_train:\n",
    "        y_pred = nn_model(X)\n",
    "        loss = loss_obj(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i+=1\n",
    "        if not i % 1000:\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[5.7460113]\n [5.7460113]\n [5.7460113]\n [5.7460113]\n [5.7460113]\n [5.7460113]\n [5.7460113]\n [5.7460113]\n [5.7460113]\n [5.7460113]]\n[[ 8.]\n [ 4.]\n [10.]\n [ 0.]\n [ 3.]\n [ 5.]\n [ 0.]\n [ 0.]\n [ 0.]\n [ 6.]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-0.010169235857269365"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "y_pred = nn_model(X_test).detach().numpy()\n",
    "y_true = y_test.detach().numpy()\n",
    "print(y_true[:10])\n",
    "print(y_pred[:10])\n",
    "r2_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}